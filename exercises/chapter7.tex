\section{Linear dependence, Linear combinations and Bases}


% Problem 3.1
\begin{problem}
  \begin{enumerate}[label=(\alph*)]
    \item Prove that the four vectors
      \[x = (1, 0, 0),\]
      \[y = (0, 1, 0),\]
      \[z = (0, 0, 1),\]
      \[u = (1, 1, 1),\]
      in $\C^3$ form a linearly dependent set, but any three of them are linearly independent.
    \item If the vectors $x, y, z,$ and $u$ in $\mathcal{P}$ are defined by $x(t) = 1, y(t) = t, z(t) = t^2,$ and $u(t) = 1 + t + t^2$, prove that $x, y, z,$ and $u$ are linearly independent, but any three of them are linearly dependent.
  \end{enumerate}
\end{problem}

\begin{solution}
  \begin{enumerate}[label=(\alph*)]
    \item Let $\alpha, \beta, \gamma, \delta$ be scalars such that
      \[\alpha x_1 + \beta y_1 + \gamma z_1 + \delta u_1 = 0,\]
      \[\alpha x_2 + \beta y_2 + \gamma z_2 + \delta u_2 = 0,\]
      \[\alpha x_3 + \beta y_3 + \gamma z_3 + \delta u_3 = 0.\]
      From here we have the following
      \[\alpha + \delta = 0,\]
      \[\beta + \delta = 0,\]
      \[\gamma + \delta = 0.\]
      And finally we see that $\alpha = \beta = \gamma = -\delta$.
      So, we have infinitely many solutions such that $\alpha \neq \beta \neq \gamma \neq \delta \neq 0$, hence $x, y, z, u$ is a linearly dependent set.
      It is also pretty clear that any three of the vectors $x, y, z, u$ are linearly independent.
    \item Let $\alpha, \beta, \gamma, \delta$ be scalars such that
      \[\alpha x(t) + \beta y(t) + \gamma z(t) + \delta u(t) = 0,\]
      for every $t$.
      If we consider $t = 0$, we see $\alpha + \delta = 0$.
      For an arbitrary $t$ we have
      \[\alpha + \beta t + \gamma t^2 + \delta + \delta t + \delta t^2\]
      or
      \[(\alpha + \delta) + (\beta + \delta)t + (\gamma + \delta)t^2 = 0.\]
      From here we see that $\alpha = \beta = \gamma = -\delta$ is a solution, so for example $(1, 1, 1, -1)$ is a solution, hence $x(t), y(t), z(t), u(t)$ are linearly dependent.
      It is again quite easy to check that any three of these vectors are linearly independent.
  \end{enumerate}
\end{solution}

% Problem 3.2
\begin{problem}
  Prove that if $\R$ is considered as a rational vector space, then a necessary and sufficient condition that the vectors $1$ and $\xi$ in $\R$ be linearly independent is that the real numer $\xi$ be irrational.
\end{problem}

\begin{solution}
  Let $\R$ be a rational vector space.
  First, assume that $1$ and $\xi$ are linearly independent, i.e., $\alpha + \beta \xi = 0$ for some $\alpha, \beta \in \Q$ implies $\alpha = \beta = 0$.
  For a contradiction, suppose that $\xi = \frac{p}{q}$ for some integers $p, q$.
  Then we could let $\beta = 1$ and $\alpha = -\frac{p}{q}$ and we would have $\alpha + \beta \xi = 0$, contradicting $\alpha = \beta = 0$, so $\xi$ must be irrational.
  This shows that it is a necessary condition.

  To show that it is sufficient, assume that $\xi$ is irrational.
  For a contradiction, assume there are some $\alpha, \beta \in \Q$ such that $\alpha + \beta \xi = 0$.
  Since $\alpha, \beta \in \Q$, we know that $\alpha = \frac{p}{q}$, $\beta = \frac{m}{n}$ for some $p, q, m, n \in \Z$.
  Then we have $\frac{p}{q} + \frac{m}{n} \xi = 0$, or $\xi = -\frac{pn}{qm}$, which is clearly rational, a contradiction.
\end{solution}

% Problem 3.3
\begin{problem}
  Is it true that if $x, y$ and $z$ are linearly independent vectors, then so also are $x + y, y + z$, and $z + x$?
\end{problem}

\begin{solution}
  Let $x, y, z$ be linearly independent vectors, i.e., $\alpha x + \beta y + \gamma z = 0$ implies $\alpha = \beta = \gamma = 0$.
  Let us consider the vectors $x + y, y + z$, and $z + x$.
  Let $\alpha, \beta, \gamma$ be scalars such that $\alpha(x + y) + \beta(y + z) + \gamma(z + x) = 0$.
  Then by distributivity we have $(\alpha + \gamma)x + (\alpha + \beta)y + (\beta + \gamma)z = 0$.
  This implies (i) $\alpha = -\gamma$, (ii) $\alpha = -\beta$, and (iii) $\beta = -\gamma$.
  Combining (i) and (iii) we clearly see $\alpha = \beta$, which in conjuction with (ii) yields $\alpha = \beta = \gamma = 0$, hence these vectors are linearly independent.
\end{solution}

% Problem 3.4
\begin{problem}
  \begin{enumerate}[label=(\alph*)]
    \item Under what conditions on the scalar $\xi$ are the vectors $(1 + \xi, 1 - \xi)$ and $(1 - \xi, 1 + \xi)$ in $\C^2$ linearly independent.
    \item Under what conditions on the scalar $\xi$ are the vectors $(\xi, 1, 0), (1, \xi, 1),$ and $(0, 1, \xi)$ in $\R^3$ linearly independent.
    \item What is the answer to (b) for $\Q^3$?
  \end{enumerate}
\end{problem}

\begin{solution}
  \begin{enumerate}[label=(\alph*)]
    \item Let $\alpha, \beta$ be scalars such that
      \[\alpha(1 + \xi) + \beta(1 - \xi) = 0,\]
      \[\alpha(1 - \xi) + \beta(1 + \xi) = 0.\]
      From here we have
      \[\alpha + \beta (\alpha - \beta)\xi = 0,\]
      \[\alpha + \beta + (\beta - \alpha)\xi = 0.\]
      Subracting these we have
      \[(2\alpha - 2\beta)\xi = 0,\]
      or
      \[(\alpha = \beta \vee \xi = 0).\]
      If $\xi = 0$, we can let $\alpha = -\beta$ and we are done.
      Otherwise, we have $\alpha = \beta$ and $\alpha + \beta = 0$, which implies $\alpha = \beta = 0$.
      So, $\xi = 0$ if these vectors are linearly dependent.
    \item Similarly to before, we have
      \begin {enumerate}[label=(\roman*)]
        \item $\alpha\xi + \beta = 0,$
        \item $\alpha + \beta\xi + \gamma = 0,$
        \item $\beta + \gamma\xi = 0.$
      \end{enumerate}
      Clearly from (i) and (iii) we have $\alpha = \gamma = \beta \cdot \frac{1}{\xi},$ if $\xi \neq 0$.
      If $\xi = 0$ these vectors are clearly dependent with $\beta = 0$ and $\alpha = -\gamma$.
      From (ii) we now get $\beta(\xi + \frac{2}{\xi}) = 0$.
      Since $\beta \neq 0$ (otherwise $\alpha = \beta = \gamma = 0$) we have $\frac{\xi^2 + 2}{\xi} = 0$, however $\xi^2 + 2 = 0$ has no solutions in $\R$.
      So, $\xi = 0$ if these vectors are linearly dependent.
    \item It would remain the same.
      There would be new values for $\xi$ in $\C^3$ however.
  \end{enumerate}
\end{solution}

% Problem 3.5
\begin{problem}
  \begin{enumerate}[label=(\alph*)]
    \item The vectors $(\xi_1, \xi_2)$ and $(\eta_1, \eta_2)$ in $\C^2$ are linearly dependent if and only if $\xi_1\eta_2 = \xi_2\eta_1$.
    \item Find a similar necessary and sufficient condition for linear independence of two vectors in $\C^3$.
      Do the same for three vectors in $\C^3$.
    \item Is there a set of three linearly independent vectors in $\C^2$.
  \end{enumerate}
\end{problem}

\begin{solution}
  \begin{enumerate}[label=(\alph*)]
    \item First, let us assume the vectors $(\xi_1, \xi_2)$ and $(\eta_1, \eta_2)$ are linearly dependent in $\C^2$.
      Then, there exists some $\alpha, \beta \neq 0 \in \C^2$ such that $\alpha\xi_1 + \beta\eta_1 = 0$, $\alpha\xi_2 + \beta\eta_2 = 0$.
      If $\xi_1 = 0$ then $\xi_2 = 0$ or $\eta_2 = 0$ (otherwise $\alpha = \beta = 0$), so $\xi_1\eta_2 = \xi_2\eta_1 = 0$.
      Otherwise we have $\alpha = -\frac{\beta\eta_1}{\xi_1}$.
      Clearly $\beta \neq 0$, otherwise $\alpha = \beta = 0$.
      Replacing $\alpha$ we have $-\frac{\beta\eta_1}{\xi_1}\xi_2 + \beta\eta_2 = 0$, or $\frac{-\eta_1\xi_2 + \eta_2\xi_1}{\xi_1} = 0$.
      So $\xi_1\eta_2 = \xi_2\eta_1$.

      Conversely, let $\xi_1\eta_2 = \xi_2\eta_1$.
      Let $\alpha = -\eta_2$ and $\beta = \xi_2$.
      Then we have $\alpha\xi_1 + \beta\eta_1 = 0$ and $\alpha\xi_2 + \beta\eta_2 = 0$.
      If $\xi_2 = -\eta_2 = 0$. then let $\alpha = 1$ and $\beta = -\frac{\xi_1}{\eta_1}$.
      If $\eta_1 = 0$ then let $\alpha = 0$ and $\beta \in \C\setminus\{0\}$.
    \item We can prove in the same manner as above that $(\xi_1, \xi_2, \xi_3)$ and $(\eta_1, \eta_2, \eta_3)$ in $\C^3$ are linearly dependent if and only if $\xi_1\eta_2 = \xi_2\eta_1 \wedge \xi_2\eta_3 = \xi_3\eta_2 \wedge \xi_3\eta_1 = \xi_1\eta_3$.
      For 3 vectors in $\C^3$ we should consider the determinant, but this has not been introduced yet.
    \item Since $\C^2$ has a basis with 2 elements, no set of more than 2 vectors can be linearly independent.
      This is proven in the next section.
  \end{enumerate}
\end{solution}

% Problem 3.6
\begin{problem}
  \begin{enumerate}[label=(\alph*)]
    \item Under what conditions on the scalars $\xi$ and $\eta$ are the vectors $(1, \xi)$ and $(1, \eta)$ in $\C^2$ linearly dependent?
    \item Under what conditions on the scalars $\xi$, $\eta$, and $\zeta$ are the vectors $(1, \xi, \xi^2)$, $(1, \eta, \eta^2)$, and $(1, \zeta, \zeta^2)$ in $\C^3$ linearly dependent?
    \item Guess and prove a generalization of (a) and (b) to $\C^n$.
  \end{enumerate}
\end{problem}

\begin{solution}
  \begin{enumerate}[label=(\alph*)]
    \item Vectors $(1, \xi)$ and $(1, \eta)$ are linearly dependent if and only $\xi = \eta$.
      If $\xi = \eta$, let $\alpha = 1$ and $\beta = -1$.
      
      Conversely, if $\alpha = -\beta$, then $\alpha(\xi - \eta) = 0$, hence $\xi = \eta$.
    \item Again, we need to consider the determinant.
      From there with a some algebra we get $\xi = \eta \vee \xi = \zeta \vee \eta = \zeta$.
    \item The general case would be $n$ vectors in $\C^n$ $(1, x_1, x_1^2, \ldots, x_1^n)$, $(1, x_2, x_2^2, \ldots, x_2^n)$, \ldots, $(1, x_n, x_n^2, \ldots, x_n^n)$ is linearly dependent if and only if for some $i \neq j$ $x_i = x_j$.
      The proof again would require determinants.
  \end{enumerate}
\end{solution}

% Problem 3.7
\begin{problem}
  \begin{enumerate}[label=(\alph*)]
    \item Find two bases in $\C^4$ such that the only vectors common to both are $(0, 0, 1, 1)$ and $(1, 1, 0, 0)$.
    \item Find two bases in $\C^4$ that have no vectors in common so that one of them contains the vectors $(1, 0, 0, 0)$ and $(1, 1, 0, 0)$ and the other contains the vectors $(1, 1, 1, 0)$ and $(1, 1, 1, 1)$.
  \end{enumerate}
\end{problem}

\begin{solution}
  \begin{enumerate}[label=(\alph*)]
    \item Take the set $\{(1, 0, 0, 0), (1, 1, 0, 0), (0, 0, 1, 1), (0, 0, 0, 1)\}$ as the first basis.
      Since it is row equivalent to the identity matrix it forms a basis.
      For the second, trivially take $(2, 0, 0, 0)$ and $(0, 0, 0, 2)$ instead of $(1, 0, 0, 0)$ and $(0, 0, 0, 1)$.
    \item For the first basis take \[\{(1, 0, 0, 0), (1, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1)\}\] and for the second \[\{(2, 0, 0, 0), (2, 2, 0, 0), (1, 1, 1, 0), (1, 1, 1, 1)\}\].
      It is easy to see that these for a basis for $\C^4$.
  \end{enumerate}
\end{solution}

% Problem 3.8
\begin{problem}
  \begin{enumerate}[label=(\alph*)]
    \item Under what conditions on the scalar $\xi$ do the vectors $(1, 1, 1)$ and $(1, \xi, \xi^2)$ form a basis of $\C^3$.
    \item Under what conditions on the scalar $\xi$ do the vectors $(0, 1, \xi)$, $(\xi, 0, 1)$, and $(\xi, 1, 1 + \xi)$ form a basis of $\C^3$.
  \end{enumerate}
\end{problem}

\begin{solution}
  \begin{enumerate}[label=(\alph*)]
    \item Under no condition can two vectors span $\C^3$, as will be proven in the next chapter.
    \item Again, under no condition since $(0, 1, \xi) + (\xi, 0, 1) = (\xi, 1, 1 + \xi)$, hence this set of vectors is linearly dependent for every $\xi \in \C$.
  \end{enumerate}
\end{solution}

% Problem 3.9
\begin{problem}
  Consider the set of all those vectors in $\C^3$ each of whose coordinates is either 0 or 1; how many different bases does this set contain?
\end{problem}

\begin{solution}
  There are 8 such vectors, $\{(0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 0, 1), (1, 1, 0), (1, 0, 0), (0, 1, 0), (1, 1, 1)\}$.
  We can trivially form 7 1-dimensional basis by just taking any non-zero vector.

  To form a 2-dimensional basis we can pick any two distinct non-zero vectors.
  We can do this in $\binom{7}{2} = 21$ ways.

  To form a dimensional basis, we can choose our 3 vectors in 29 ways (check by hand to see which are linearly dependent and repeat).
  In total we have 57 basis.
\end{solution}

% Problem 3.10
\begin{problem}
  If $\mathcal{X}$ is the set consisting of the six vectors $(1, 1, 0, 0), (1, 0, 1, 0), (1, 0, 0, 1), (0, 1, 1, 0), (0, 1, 0, 1), (0, 0, 1, 1)$ in $\C^4$, find two different maximal linearly independent subsets of $\mathcal{X}$.
\end{problem}

\begin{solution}
  Any set of 4 linearly independent vectors in $\C^4$ is maximally linearly independent.
  Two such sets are $\{(1, 1, 0, 0), (1, 0, 1, 0), (0, 0, 1, 1), (0, 1, 1, 0)\}$ and $\{(1, 1, 0, 0), (1, 0, 1, 0), (0, 0, 1, 1), (1, 0, 0, 1)\}$.
  It is easy to show their independence.
\end{solution}

% Problem 3.11
\begin{problem}
  Prove that every vector space has a basis.
\end{problem}

\begin{solution}
  Let $\mathcal{V}$ be a set of vectors.
  Consider the set of all linearly independent subsets of $\mathcal{V}$, call this set $\mathcal{V}_L$.
  Also, consider a partial ordering on $\mathcal{V}_L$ given by $A < B \Leftrightarrow A \subset B$.
  Now, let us look at a totally ordered subset of $A$ of $\mathcal{V}_L$.
  Clearly $\bigcup A$ is an upper bound of $A$, we shall show that $\bigcup A \in \mathcal{V}_L$.
  Consider a finite subset of $\mathcal{V}_L$, call it $X = \{x_1, x_2, \ldots, x_n\}$.
  By the definition of a union there must be some $X_1, \ldots, X_n \in A$ such that $x_1 \in X_1, \ldots, x_n \in X_n$.
  Since $A$ is totally ordered any finite subset of $A$ must have a greatest element.
  Call it $Y$.
  Clearly $X \subset Y$ and $Y$ must be linearly independent since $Y \in A \in \mathcal{V}_L$.
  Hence $X$ is linearly independent.
  Since $X$ was arbitrary, any finite subset of $\bigcup A$ must be linearly independent, thus $\bigcup A$ is linearly independent.
  Hence $\bigcup A$ is an upper bound of $A$ in $\mathcal{V}_L$.
  Since $A$ was arbitrary, by Zorn's lemma, $\mathcal{V}_L$ has a maximal element, call it $a$.
  Clearly $a$ is linearly independent and must span $\mathcal{V}$, since if we had some $b \in \mathcal{V}$ such that $b$ couldn't be written as a linear combination of vectors in $a$, then $a \cup b$ would be linearly independent, contradicting the maximality of $a$.
  Hence $a$ is a basis of $\mathcal{V}$.
  Since $\mathcal{V}$ was arbitrary, every vector space has a basis.
\end{solution}
